{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cd9faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "\n",
    "from run_match import get_region_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0021fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "def load_data():\n",
    "    ex = pd.read_csv(\"ex.csv\",delimiter=\";\")\n",
    "    ex.columns = [\"ext_original_name\", \"ext_nation\", \"ext_region\"]\n",
    "\n",
    "    inter = pd.read_csv(\"int.csv\",delimiter=\";\")\n",
    "    inter.columns = [\"customer_id\",\"int_original_name\", \"int_nation\", \"int_region\"]\n",
    "\n",
    "    # remove the company type from the name\n",
    "    ex['clean_name'] = remove_words(ex['ext_original_name'])\n",
    "\n",
    "    # inter split the region from name\n",
    "    int_name_region = inter[\"int_original_name\"].str.split(',', expand=True)\n",
    "    inter['clean_name'] = remove_words(int_name_region[0])\n",
    "    # inter[\"int_sub_region\"] = int_name_region[1]\n",
    "    del int_name_region\n",
    "\n",
    "    # drop the duplicated rows\n",
    "    clean_ex = ex.drop_duplicates()\n",
    "    clean_int = inter.drop_duplicates()\n",
    "\n",
    "    # reomve the unkonow company\n",
    "    clean_ex = clean_ex.loc[clean_ex[\"clean_name\"]!=\"unknown\", :]\n",
    "    clean_int = clean_int.loc[clean_int[\"clean_name\"]!=\"unknown\", :]\n",
    "    return clean_ex, clean_int\n",
    "\n",
    "\n",
    "def remove_words(origina_name_series): \n",
    "    # lower the word\n",
    "    origina_name_series = origina_name_series.str.lower()\n",
    "    \n",
    "    # remove the special characers\n",
    "    origina_name_series = origina_name_series.str.replace('[^\\w\\s]', '', regex=True)\n",
    "        \n",
    "    remove_words = [\"llc\", \"ltd\", 'limited',\"co\", 'corp',\"inc\",'bv',\n",
    "                    'holding','holdings', 'plc', 'group', 'bvba','sa']\n",
    "    remove_words = r'\\b(?:{})\\b'.format('|'.join(remove_words))\n",
    "    new_name_series = origina_name_series.str.replace(remove_words, '', regex=True)\n",
    "    \n",
    "    # remove the extra space\n",
    "    new_name_series = new_name_series.str.replace(r'\\s+',' ', regex=True)\n",
    "    new_name_series = new_name_series.str.strip()\n",
    "    \n",
    "    return new_name_series\n",
    "\n",
    "\n",
    "# count the freq words\n",
    "def get_word_freq(series, top=20):\n",
    "    return pd.Series(' '.join(series).split()).value_counts()[:top]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11151c16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pandas.core.strings.accessor.StringMethods object at 0x105fbe730>\n",
      "<pandas.core.strings.accessor.StringMethods object at 0x13e0ee730>\n",
      "<pandas.core.strings.accessor.StringMethods object at 0x13e5ea730>\n",
      "<pandas.core.strings.accessor.StringMethods object at 0x1428d2730>\n",
      "<pandas.core.strings.accessor.StringMethods object at 0x110c0d730>\n",
      "<pandas.core.strings.accessor.StringMethods object at 0x139edd730>\n",
      "<pandas.core.strings.accessor.StringMethods object at 0x10804e730>\n",
      "<pandas.core.strings.accessor.StringMethods object at 0x13f096730>\n",
      "<pandas.core.strings.accessor.StringMethods object at 0x14118b730>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_match(clean_ex, clean_int):\n",
    "    match_ration_dic = {\"ratio\":fuzz.ratio,\n",
    "                        \"token_sort_ratio\":fuzz.token_sort_ratio}\n",
    "\n",
    "    all_region_result = []\n",
    "    for region, single_region_ex in tqdm(clean_ex.groupby(\"ext_region\")):\n",
    "        region_int = clean_int.loc[clean_int[\"int_region\"]==region]\n",
    "        \n",
    "        # mutil-core match\n",
    "        n_cores = (mp.cpu_count() - 1)\n",
    "        split_df = np.array_split(single_region_ex, n_cores)\n",
    "        in_arg = []\n",
    "        for i in range(len(split_df)):\n",
    "            if split_df[i].size >0:\n",
    "                in_arg.append([split_df[i], region_int, match_ration_dic])\n",
    "            else:\n",
    "                in_arg.append([pd.DataFrame(columns=single_region_ex.columns),\n",
    "                             region_int, match_ration_dic])\n",
    "\n",
    "        with mp.Pool(processes = n_cores) as multi_process:\n",
    "            results = multi_process.starmap(get_region_match, in_arg)\n",
    "        single_region_rs = pd.concat(results)\n",
    "        assert len(single_region_rs) == len(single_region_ex)\n",
    "        \n",
    "        single_region_rs = single_region_rs.merge(\n",
    "                  region_int, \n",
    "                  left_on = f\"final_match\", \n",
    "                  right_on = \"clean_name\",\n",
    "                  how=\"left\",\n",
    "                  suffixes=('', f'_matchedby')).drop(\n",
    "                        f'clean_name_matchedby',axis=1)\n",
    "        all_region_result.append(single_region_rs)\n",
    "\n",
    "    # concat different region results\n",
    "    match_df = pd.concat(all_region_result)\n",
    "    \n",
    "    # save the result\n",
    "    print(\"save results ...\")\n",
    "    internal_info_col = [\"final_match\",\"customer_id\", \"int_original_name\", \"int_nation\", \"int_region\"]\n",
    "    final_mathch = match_df[clean_ex.columns.tolist() + internal_info_col].dropna(\n",
    "        subset=[\"final_match\"], how='all').drop([\"clean_name\",\"final_match\"],axis=1)\n",
    "\n",
    "    \n",
    "    return final_mathch, match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a57c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ex, clean_int = load_data()\n",
    "\n",
    "final_mathched, match_details = run_match(clean_ex, clean_int)\n",
    "\n",
    "final_mathched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96f87ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mathched.to_excel(\"final_mathched.xlsx\",index=False) \n",
    "match_result_info.to_excel(\"match_result_info.xlsx\",index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1bf721c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mathched.drop([\"clean_name\",\"final_match\"],axis=1).to_excel(\"final_mathched.xlsx\",index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff500a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
